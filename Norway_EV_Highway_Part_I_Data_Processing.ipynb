{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzm-6Oa9U3J2"
   },
   "source": [
    "# Google Trends insight in understanding what drives Norwegians into purchasing an electric vehicle\n",
    "Authors: \n",
    "- 270226 - Max Chevron \n",
    "- 321733 - Shasha Jiang \n",
    "- 314205 - Jostein Leirgulen \n",
    "\n",
    "Date: 03.12.2020 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3619,
     "status": "ok",
     "timestamp": 1607299918116,
     "user": {
      "displayName": "sha jiang",
      "photoUrl": "",
      "userId": "03234591794427525118"
     },
     "user_tz": -60
    },
    "id": "Co3K6ojFU3J2",
    "outputId": "96efef8f-527d-41a9-ebb5-c1ebf0cac16e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd \n",
    "from dateutil import relativedelta as rdelta\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.stats import diagnostic\n",
    "import statsmodels.formula.api as smf\n",
    "from pytrends.request import TrendReq\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import statsmodels.regression.linear_model as lm\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U879jLCTU3J3"
   },
   "source": [
    "## Step 1 - Data wrangling\n",
    "\n",
    "### Step 1.1 - EV sales and market share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3613,
     "status": "ok",
     "timestamp": 1607299918120,
     "user": {
      "displayName": "sha jiang",
      "photoUrl": "",
      "userId": "03234591794427525118"
     },
     "user_tz": -60
    },
    "id": "9DaIq39LU3J3"
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/EV_sales_norway.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 3936,
     "status": "ok",
     "timestamp": 1607299918453,
     "user": {
      "displayName": "sha jiang",
      "photoUrl": "",
      "userId": "03234591794427525118"
     },
     "user_tz": -60
    },
    "id": "yG3HAky8U3J3",
    "outputId": "97ec30fe-3bda-4b17-d11b-d892fcc95877"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Jostein\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  \"\"\"\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>other</th>\n",
       "      <th>electric</th>\n",
       "      <th>weekday</th>\n",
       "      <th>ev_share</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2010</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-03</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2010</td>\n",
       "      <td>2</td>\n",
       "      <td>2776</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-10</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2010</td>\n",
       "      <td>3</td>\n",
       "      <td>1953</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-17</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2010</td>\n",
       "      <td>4</td>\n",
       "      <td>2078</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-24</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2010</td>\n",
       "      <td>5</td>\n",
       "      <td>2873</td>\n",
       "      <td>0</td>\n",
       "      <td>2010-01-31</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>572</th>\n",
       "      <td>2020</td>\n",
       "      <td>44</td>\n",
       "      <td>1749</td>\n",
       "      <td>1869</td>\n",
       "      <td>2020-11-01</td>\n",
       "      <td>0.516584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>573</th>\n",
       "      <td>2020</td>\n",
       "      <td>45</td>\n",
       "      <td>961</td>\n",
       "      <td>1291</td>\n",
       "      <td>2020-11-08</td>\n",
       "      <td>0.573268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>574</th>\n",
       "      <td>2020</td>\n",
       "      <td>46</td>\n",
       "      <td>1174</td>\n",
       "      <td>1435</td>\n",
       "      <td>2020-11-15</td>\n",
       "      <td>0.550019</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>575</th>\n",
       "      <td>2020</td>\n",
       "      <td>47</td>\n",
       "      <td>1259</td>\n",
       "      <td>1591</td>\n",
       "      <td>2020-11-22</td>\n",
       "      <td>0.558246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>576</th>\n",
       "      <td>2020</td>\n",
       "      <td>48</td>\n",
       "      <td>1586</td>\n",
       "      <td>2084</td>\n",
       "      <td>2020-11-29</td>\n",
       "      <td>0.567847</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>577 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     year  week  other  electric    weekday  ev_share\n",
       "0    2010     1     17         0 2010-01-03  0.000000\n",
       "1    2010     2   2776         0 2010-01-10  0.000000\n",
       "2    2010     3   1953         0 2010-01-17  0.000000\n",
       "3    2010     4   2078         0 2010-01-24  0.000000\n",
       "4    2010     5   2873         0 2010-01-31  0.000000\n",
       "..    ...   ...    ...       ...        ...       ...\n",
       "572  2020    44   1749      1869 2020-11-01  0.516584\n",
       "573  2020    45    961      1291 2020-11-08  0.573268\n",
       "574  2020    46   1174      1435 2020-11-15  0.550019\n",
       "575  2020    47   1259      1591 2020-11-22  0.558246\n",
       "576  2020    48   1586      2084 2020-11-29  0.567847\n",
       "\n",
       "[577 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sales['weekday'] = ''\n",
    "pd.to_datetime(sales['weekday'])\n",
    "for index, row in sales.iterrows():\n",
    "    # minus 1 because the dataset week system goes from 1 to 53 instead of 0 to 52, day start from sunday 0 to macth google trend\n",
    "    sales.weekday[index] = datetime.strptime('{} {} 0'.format(row.year, row.week-1), '%Y %W %w').strftime(\"%Y-%m-%d\") \n",
    "sales.weekday = pd.to_datetime(sales.weekday, format='%Y-%m-%d')\n",
    "#compute market share for electric cars\n",
    "sales['ev_share'] = sales['electric']/(sales['other'] + sales['electric'])\n",
    "sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNgumZreU3J6"
   },
   "source": [
    "### Step 1.2 - Google trend data import and wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function goal:      Convert start and end date to a string of dates with specific format\n",
    "def convert_to_date(satrt,end):\n",
    "      start = satrt.strftime(\"%Y-%m-%d\")\n",
    "      end = end.strftime(\"%Y-%m-%d\")\n",
    "      dates = start + ' ' + end\n",
    "      return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function goal:      Split timespan in years\n",
    "Function inputs:    date_start\n",
    "                    date_end\n",
    "Function outputs:   years_expand = Array years\n",
    "'''\n",
    "\n",
    "def split_data_into_year(date_start, date_end):\n",
    "    years_expand = []\n",
    "    start_year = date_start\n",
    "    while abs(rdelta.relativedelta(start_year,date_end).years) >0 :\n",
    "      one_year = rdelta.relativedelta(weeks=+50)\n",
    "      end_year = start_year + one_year\n",
    "    \n",
    "      years_expand += [convert_to_date(start_year,end_year)]\n",
    "\n",
    "      #update start_year\n",
    "      one_week = rdelta.relativedelta(weeks=+1)\n",
    "      start_year = end_year + one_week\n",
    "\n",
    "    years_expand += [convert_to_date(start_year,date_end)]\n",
    "    return years_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function goal:      Import Google Trends data\n",
    "Function inputs:    kw = Keyword list \n",
    "                    geo = Country abreviation to search\n",
    "                    dates = Timespan to search\n",
    "                    eps = maximum propensity difference in matching\n",
    "Function outputs:   pytrends = full pytrends format dataset\n",
    "                    interest_over_time_df = Dataframe containing the interest evolution for each keyword\n",
    "'''\n",
    "\n",
    "def build_GT_data(kw, geo, dates):\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            pytrends.build_payload(kw_list=kw, cat=0, timeframe=dates, geo=geo, gprop='')\n",
    "            break\n",
    "        except:    \n",
    "            pass # Sometimes Google Trends rejects the request. Try again until it works ! \n",
    "\n",
    "    interest_over_time_df = pytrends.interest_over_time()\n",
    "\n",
    "    return interest_over_time_df[kw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function goal:      Normalise the weekly values. It finds the date and keyword with the max value in weekly. \n",
    "                    Looks for the value in the monthly dataset which is smaller and normalises all the weekly value to this one.\n",
    "Function inputs:    data_weekly = 1 year of Google Trends data with weekly granularity\n",
    "                    data_monthly = Whole deseired timespan of Google Trends data with monthly granularity\n",
    "                    kw = keywords\n",
    "Function outputs:   data_weekly = same as input but normalised \n",
    "'''\n",
    "def normalise(data_weekly, data_monthly, kw):\n",
    "    # 1) Convert weekly into monthly with mean()\n",
    "    data = data_weekly.groupby(data_weekly.index.strftime(\"%Y-%m\")).mean()\n",
    "\n",
    "    # 2) Find row index and column name of max value (which is not 100 as we have done the mean over 4 weeks)\n",
    "    yearly_max = data.max().max()\n",
    "    max_column = data.max().idxmax()\n",
    "    max_row = data[max_column].idxmax()\n",
    "\n",
    "    # 3) For the whole timespan (with monhtly datapoints), the value at the weekly max is 'true_max' \n",
    "    true_max = data_monthly[max_column].loc[data_monthly.index > max_row].iloc[0]\n",
    "\n",
    "    # 4) Normalise each weekly datapoints by this true_max/yearly_max in %\n",
    "    data_weekly = data_weekly*(true_max/yearly_max)/100\n",
    "\n",
    "    return data_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function goal:      Split the timespan in years so that we can extract weekly datapoints. \n",
    "Function inputs:    data_weekly = 1 year of Google Trends data with weekly granularity\n",
    "                    interest_monthly = Whole deseired timespan of Google Trends data with monthly granularity\n",
    "                    kw = keywords\n",
    "                    yrs = full timespan split into years 'start_date end_date' for each year as a list\n",
    "                    geo = Country abreviation to search\n",
    "Function outputs:   interest = full timespan interest with weekly datapoints\n",
    "'''\n",
    "def get_weekly_interest(interest_monthly, kw, yrs, geo):\n",
    "    interest = pd.DataFrame()\n",
    "    for yearly_dates in yrs:\n",
    "        interest_weekly = build_GT_data(kw, geo, yearly_dates)\n",
    "        \n",
    "        #if the search is for very short timespan it will result in daily instead of weekly values. So we take the weekly mean in case this happens\n",
    "        interest_weekly['date'] = pd.to_datetime(interest_weekly.index) - pd.to_timedelta(7, unit='d')\n",
    "        interest_weekly = interest_weekly.groupby(pd.Grouper(key='date', freq='W-SUN')).mean()\n",
    "        \n",
    "        interest_weekly = normalise(interest_weekly, interest_monthly, kw)\n",
    "        interest = pd.concat([interest, interest_weekly])\n",
    "    #The max week might actually not be around the same date as the max month. Hence we need to normalise again here so that max value is 100\n",
    "    interest = interest/interest.max().max()*100\n",
    "    return interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Eco keywords: [\"CO2 bil\", \"Bil utslipp\", \"Global oppvarming\", \"Bil miljÃ¸\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTC_offset = 60 # Minutes for Norway (UTC+1)\n",
    "host_language = 'en_US'\n",
    "# keywords related to category: economical \n",
    "kw_eco = [\"Elbil bompenger\", \"Elbil avgift\", \"Elbil pris\", \"Elbil fordeler\", \"Elbil parkering\"] # REMOVED:, \"Bompenger\", \"Bompenger pris\", \"Parkering pris\", ] \n",
    "# keywords related to category: EV models (top 10 EV models sold in Norway with Tesla models grouped together as 'Tesla')\n",
    "kw_evm = [\"Tesla\", \"Nissan Leaf\",\"Volkswagen e-Golf\", \"Bmw I3\", \"Kia Soul\"] # REMOVED:, \"Renault Zoe\", \"Audi E-tron\", \"Hyundai Ioniq\"] \n",
    "# keywords related to category: range anxiety \n",
    "kw_ran = [\"Elbil lading\", \"Elbil ladestasjoner\", \"Ladestasjoner\", \"Hurtiglading\", \"Elbil rekkevidde\"] # REMOVED:, \"Tesla supercharger\"] \n",
    "# keywords related to category: environement \n",
    "#kw_env = [\"Karbonavtrykk\", \"Luftkvalitet\", \"CO2\", \"CO2 bil\", \"CO2 utslipp\"] \n",
    "kw_env = [\"CO2 bil\", \"Bil utslipp\", \"Global oppvarming\", \"Bil miljÃ¸\", \"Karbonavtrykk\"]\n",
    "# all keywords grouped\n",
    "kw_all = kw_eco + kw_evm + kw_ran + kw_env\n",
    "#date_start = date(2009,12,28) # Sales dataset Timespan\n",
    "#date_end = date(2020,11,29)\n",
    "date_start = date(2010,1,3)\n",
    "date_end = date(2020,1,5)\n",
    "dates = convert_to_date(date_start,date_end)\n",
    "yrs = split_data_into_year(date_start, date_end) # Will be usefull to extract yearly datasets to have weekly instead of monthly datapoints\n",
    "country_abbreviation = 'NO' #Norway abreviation\n",
    "pytrends = TrendReq(hl=host_language, tz=UTC_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: get non granular over results for reference:\n",
    "# Giving a too large timespan yields unfortunately only monthy results. This will however serve as our reference frame for normalisation\n",
    "interest_monthly_kw_eco = build_GT_data(kw_eco, country_abbreviation, dates)\n",
    "interest_monthly_kw_evm = build_GT_data(kw_evm, country_abbreviation, dates)\n",
    "interest_monthly_kw_ran = build_GT_data(kw_ran, country_abbreviation, dates)\n",
    "interest_monthly_kw_env = build_GT_data(kw_env, country_abbreviation, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: get weekly \n",
    "interest_kw_eco = get_weekly_interest(interest_monthly_kw_eco, kw_eco, yrs, country_abbreviation)\n",
    "interest_kw_evm = get_weekly_interest(interest_monthly_kw_evm, kw_evm, yrs, country_abbreviation)\n",
    "\n",
    "#NOTE: GOOGLE DOES WEIRD THINGS SOMETIMES. SO YOU MIGHT NEED TO RUN A LINE AGAIN INDIVIDUALLY IF YOU HAVE AN ERROR\n",
    "#For me it seems like its mainly for 'ran' that it fails but after a while if you run this line individually it works\n",
    "# When you do too many queries you get rejected by Google Trends. You need to pause before starting again to avoid errors. 60 sec is recommended by pytrends\n",
    "# Doesn't seem to work though ...\n",
    "#time.sleep(60) \n",
    "\n",
    "interest_kw_ran = get_weekly_interest(interest_monthly_kw_ran, kw_ran, yrs, country_abbreviation)\n",
    "interest_kw_env = get_weekly_interest(interest_monthly_kw_env, kw_env, yrs, country_abbreviation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7t9OHNVQU3J6"
   },
   "source": [
    "### Step 1.3 - Key events definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "executionInfo": {
     "elapsed": 16030,
     "status": "ok",
     "timestamp": 1607299930623,
     "user": {
      "displayName": "sha jiang",
      "photoUrl": "",
      "userId": "03234591794427525118"
     },
     "user_tz": -60
    },
    "id": "TdSpuRrnU3J6"
   },
   "outputs": [],
   "source": [
    "# Economical related dates which penalises combustion vehicles (in favor of EV)\n",
    "dates_eco = {\"Climate settlement announcement\":\"2012-04-24\",\n",
    "             \"Kyoto agreement second term\":\"2013-05-31\"}\n",
    "\n",
    "# EV model key releas dates\n",
    "# Unknown date of the month for dates_evm so set to 15\n",
    "dates_evm = {\"Tesla model 3 first release\":\"2019-07-15\",\n",
    "             \"Nissan Leaf first release\":\"2010-12-15\"}\n",
    "\n",
    "# Range anxiety mitigation key dates\n",
    "dates_ran = {\"Tesla supercharging system start\":\"2013-07-01\"}\n",
    "\n",
    "# Rising environmental awarness key dates\n",
    "dates_env = {\"Paris agreement anouncement\":\"2015-01-04\",\n",
    "             \"Paris agreement updated emission cuts\":\"2020-02-27\"}\n",
    "\n",
    "# All dates together\n",
    "dates_all = dict(dates_eco)\n",
    "dates_all.update(dates_evm)\n",
    "dates_all.update(dates_ran)\n",
    "dates_all.update(dates_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "executionInfo": {
     "elapsed": 16028,
     "status": "ok",
     "timestamp": 1607299930628,
     "user": {
      "displayName": "sha jiang",
      "photoUrl": "",
      "userId": "03234591794427525118"
     },
     "user_tz": -60
    },
    "id": "BBMOku8daMw5"
   },
   "outputs": [],
   "source": [
    "sales_base = sales.copy()\n",
    "\n",
    "# convert rows to month or year where each row is 1 week\n",
    "#month_delay = 4\n",
    "year_delay = 52\n",
    "\n",
    "#a fix - t-1 should not be 4, which would imply we are trying to use last years week, and four weeks past in order \n",
    "# to predict NOW\n",
    "week_delay = 1\n",
    "\n",
    "# BOTH LOG AND LINEAR COLUMNS HERE. REMOVE ONE OR THE OTHER WHEN DECIDED: \n",
    "sales_base['ev_share_tm1']=sales_base.ev_share.shift(week_delay)\n",
    "sales_base['ev_share_tm12']=sales_base.ev_share.shift(year_delay)\n",
    "#replace 0 to a samll number bigger than 1 so that log can be defined\n",
    "#sales_base['log_ev_share'] = np.log(sales_base.ev_share.replace(0, 0.5))\n",
    "#sales_base['log_ev_share_tm1']=sales_base.log_ev_share.shift(month_delay)\n",
    "#sales_base['log_ev_share_tm12']=sales_base.log_ev_share.shift(year_delay)\n",
    "\n",
    "### Modifications by JOSTEIN\n",
    "sales_base['log_ev_share'] = np.log(sales_base.ev_share + 0.1)\n",
    "sales_base['log_ev_share_tm1']=sales_base.log_ev_share.shift(week_delay)\n",
    "sales_base['log_ev_share_tm12']=sales_base.log_ev_share.shift(year_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes for statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = sales_base.iloc[:528,:].drop_duplicates(subset='weekday').set_index('weekday')\n",
    "df_inter_eco = interest_kw_eco.iloc[1:,:]\n",
    "df_inter_evm = interest_kw_evm.iloc[1:,:]\n",
    "df_inter_ran = interest_kw_ran.iloc[1:,:]\n",
    "df_inter_env = interest_kw_env.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Elbil bompenger</th>\n",
       "      <th>Elbil avgift</th>\n",
       "      <th>Elbil pris</th>\n",
       "      <th>Elbil fordeler</th>\n",
       "      <th>Elbil parkering</th>\n",
       "      <th>Tesla</th>\n",
       "      <th>Nissan Leaf</th>\n",
       "      <th>Volkswagen e-Golf</th>\n",
       "      <th>Bmw I3</th>\n",
       "      <th>Kia Soul</th>\n",
       "      <th>Elbil lading</th>\n",
       "      <th>Elbil ladestasjoner</th>\n",
       "      <th>Ladestasjoner</th>\n",
       "      <th>Hurtiglading</th>\n",
       "      <th>Elbil rekkevidde</th>\n",
       "      <th>CO2 bil</th>\n",
       "      <th>Bil utslipp</th>\n",
       "      <th>Global oppvarming</th>\n",
       "      <th>Bil miljÃ¸</th>\n",
       "      <th>Karbonavtrykk</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>date</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-01-03</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.274256</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>40.214559</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-10</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>49.042146</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-17</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.243917</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.616858</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-24</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.425287</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-01-31</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.394413</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.808429</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-01</th>\n",
       "      <td>4.046915</td>\n",
       "      <td>2.023457</td>\n",
       "      <td>3.974649</td>\n",
       "      <td>1.951191</td>\n",
       "      <td>10.189554</td>\n",
       "      <td>32.644628</td>\n",
       "      <td>2.479339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.413223</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.814528</td>\n",
       "      <td>2.926284</td>\n",
       "      <td>25.064260</td>\n",
       "      <td>5.470879</td>\n",
       "      <td>5.216420</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>15.126525</td>\n",
       "      <td>37.049026</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-08</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.648503</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>4.046915</td>\n",
       "      <td>27.272727</td>\n",
       "      <td>2.066116</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.740812</td>\n",
       "      <td>5.343649</td>\n",
       "      <td>19.338921</td>\n",
       "      <td>8.524393</td>\n",
       "      <td>8.397163</td>\n",
       "      <td>15.784200</td>\n",
       "      <td>7.453650</td>\n",
       "      <td>36.172126</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-15</th>\n",
       "      <td>4.408247</td>\n",
       "      <td>2.457055</td>\n",
       "      <td>8.599694</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.695418</td>\n",
       "      <td>28.512397</td>\n",
       "      <td>2.479339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.826446</td>\n",
       "      <td>0.413223</td>\n",
       "      <td>6.234257</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>19.847840</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>14.376961</td>\n",
       "      <td>7.234425</td>\n",
       "      <td>7.672875</td>\n",
       "      <td>37.925926</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-22</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>10.117287</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.384789</td>\n",
       "      <td>32.644628</td>\n",
       "      <td>1.652893</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.652893</td>\n",
       "      <td>0.413223</td>\n",
       "      <td>25.445949</td>\n",
       "      <td>6.361487</td>\n",
       "      <td>31.044058</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019-12-29</th>\n",
       "      <td>4.046915</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.793036</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.359438</td>\n",
       "      <td>28.925620</td>\n",
       "      <td>2.479339</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.826446</td>\n",
       "      <td>15.522029</td>\n",
       "      <td>3.307973</td>\n",
       "      <td>14.504191</td>\n",
       "      <td>8.778852</td>\n",
       "      <td>19.975070</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>7.892100</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.234425</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            Elbil bompenger  Elbil avgift  Elbil pris  Elbil fordeler  \\\n",
       "date                                                                    \n",
       "2010-01-03         0.000000      0.000000    0.000000        0.000000   \n",
       "2010-01-10         0.000000      0.000000    0.000000        0.000000   \n",
       "2010-01-17         0.000000      0.000000    0.000000        0.000000   \n",
       "2010-01-24         0.000000      0.000000    0.000000        0.000000   \n",
       "2010-01-31         0.000000      0.000000    0.000000        0.000000   \n",
       "...                     ...           ...         ...             ...   \n",
       "2019-12-01         4.046915      2.023457    3.974649        1.951191   \n",
       "2019-12-08         0.000000      0.000000    6.648503        0.000000   \n",
       "2019-12-15         4.408247      2.457055    8.599694        0.000000   \n",
       "2019-12-22         0.000000      0.000000   10.117287        0.000000   \n",
       "2019-12-29         4.046915      0.000000    6.793036        0.000000   \n",
       "\n",
       "            Elbil parkering      Tesla  Nissan Leaf  Volkswagen e-Golf  \\\n",
       "date                                                                     \n",
       "2010-01-03         0.000000   1.274256     0.000000                0.0   \n",
       "2010-01-10         0.000000   0.000000     0.000000                0.0   \n",
       "2010-01-17         0.000000   1.243917     0.000000                0.0   \n",
       "2010-01-24         0.000000   0.000000     0.000000                0.0   \n",
       "2010-01-31         0.000000   0.394413     0.000000                0.0   \n",
       "...                     ...        ...          ...                ...   \n",
       "2019-12-01        10.189554  32.644628     2.479339                0.0   \n",
       "2019-12-08         4.046915  27.272727     2.066116                0.0   \n",
       "2019-12-15        10.695418  28.512397     2.479339                0.0   \n",
       "2019-12-22         2.384789  32.644628     1.652893                0.0   \n",
       "2019-12-29         6.359438  28.925620     2.479339                0.0   \n",
       "\n",
       "              Bmw I3  Kia Soul  Elbil lading  Elbil ladestasjoner  \\\n",
       "date                                                                \n",
       "2010-01-03  0.000000  0.000000      0.000000             0.000000   \n",
       "2010-01-10  0.000000  0.000000      0.000000             0.000000   \n",
       "2010-01-17  0.000000  0.000000      0.000000             0.000000   \n",
       "2010-01-24  0.000000  0.000000      0.000000             0.000000   \n",
       "2010-01-31  0.000000  0.000000      0.000000             0.000000   \n",
       "...              ...       ...           ...                  ...   \n",
       "2019-12-01  0.413223  0.000000     10.814528             2.926284   \n",
       "2019-12-08  0.000000  0.000000     13.740812             5.343649   \n",
       "2019-12-15  0.826446  0.413223      6.234257             0.000000   \n",
       "2019-12-22  1.652893  0.413223     25.445949             6.361487   \n",
       "2019-12-29  0.000000  0.826446     15.522029             3.307973   \n",
       "\n",
       "            Ladestasjoner  Hurtiglading  Elbil rekkevidde    CO2 bil  \\\n",
       "date                                                                   \n",
       "2010-01-03       0.000000      0.000000          0.000000   0.000000   \n",
       "2010-01-10       0.000000      0.000000          0.000000   0.000000   \n",
       "2010-01-17       0.000000      0.000000          0.000000   0.000000   \n",
       "2010-01-24       0.000000      0.000000          0.000000   0.000000   \n",
       "2010-01-31       0.000000      0.000000          0.000000   0.000000   \n",
       "...                   ...           ...               ...        ...   \n",
       "2019-12-01      25.064260      5.470879          5.216420   0.000000   \n",
       "2019-12-08      19.338921      8.524393          8.397163  15.784200   \n",
       "2019-12-15      19.847840      0.000000         14.376961   7.234425   \n",
       "2019-12-22      31.044058      0.000000          0.000000   0.000000   \n",
       "2019-12-29      14.504191      8.778852         19.975070   0.000000   \n",
       "\n",
       "            Bil utslipp  Global oppvarming  Bil miljÃ¸  Karbonavtrykk  \n",
       "date                                                                  \n",
       "2010-01-03     0.000000          40.214559        0.0       0.000000  \n",
       "2010-01-10     0.000000          49.042146        0.0       0.000000  \n",
       "2010-01-17     0.000000          19.616858        0.0       0.000000  \n",
       "2010-01-24     0.000000          29.425287        0.0       0.000000  \n",
       "2010-01-31     0.000000           9.808429        0.0       0.000000  \n",
       "...                 ...                ...        ...            ...  \n",
       "2019-12-01    15.126525          37.049026        0.0       0.000000  \n",
       "2019-12-08     7.453650          36.172126        0.0       0.000000  \n",
       "2019-12-15     7.672875          37.925926        0.0       0.000000  \n",
       "2019-12-22     0.000000           0.000000        0.0       0.000000  \n",
       "2019-12-29     0.000000           7.892100        0.0       7.234425  \n",
       "\n",
       "[522 rows x 20 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_inter = df_inter_eco.merge(df_inter_evm, left_index=True, right_index=True)\n",
    "df_inter = df_inter.merge(df_inter_ran, left_index=True, right_index=True)\n",
    "df_inter = df_inter.merge(df_inter_env, left_index=True, right_index=True)\n",
    "df_inter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_inter.to_pickle('pytrends_V2.pickle')\n",
    "#df_sales.to_pickle('sales_V2.pickle')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Norway_EV_sales.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
