{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dzm-6Oa9U3J2"
   },
   "source": [
    "# Norway EV Highway - Part I - Data Processing\n",
    "## What drives people to buy EV cars?\n",
    "Authors: Team CROWS composed by\n",
    "- 314205 - Jostein Leirgulen \n",
    "- 270226 - Max Chevron \n",
    "- 321733 - Shasha Jiang \n",
    "\n",
    "Date: 18.12.2020 \n",
    "\n",
    "### Purpose of this notebook: \n",
    "Import two datasets, wrangle the data to our convenience and store them in pickle files to be used by the subsequent notebook \"Norway_EV_Highway_Part_II_Data_Analysis\"\n",
    "\n",
    "1) Opplysningsrådet for Veitrafikk - Road Traffic Information Council \n",
    "\n",
    "- https://ofv.no/\n",
    "- Weekly sales of all vehicles as well as EV in Norway between 2010 and 2020. \n",
    "\n",
    "2) Google Trends:\n",
    "\n",
    "- https://trends.google.com/trends\n",
    "- For the same time window we will import historical data on selected Norwegian keywords which represent best the 4 categories we will investigate, namely: \n",
    "  - Economical factors\n",
    "  - New EV model releases\n",
    "  - Range anxiety\n",
    "  - Environmental concerns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3619,
     "status": "ok",
     "timestamp": 1607299918116,
     "user": {
      "displayName": "sha jiang",
      "photoUrl": "",
      "userId": "03234591794427525118"
     },
     "user_tz": -60
    },
    "id": "Co3K6ojFU3J2",
    "outputId": "96efef8f-527d-41a9-ebb5-c1ebf0cac16e"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import pandas as pd \n",
    "from dateutil import relativedelta as rdelta\n",
    "from datetime import datetime, date\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from statsmodels.stats import diagnostic\n",
    "import statsmodels.formula.api as smf\n",
    "from pytrends.request import TrendReq\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "import statsmodels.regression.linear_model as lm\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U879jLCTU3J3"
   },
   "source": [
    "## Step 1 - Data wrangling\n",
    "\n",
    "### Step 1.1 - EV sales and market share"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 3613,
     "status": "ok",
     "timestamp": 1607299918120,
     "user": {
      "displayName": "sha jiang",
      "photoUrl": "",
      "userId": "03234591794427525118"
     },
     "user_tz": -60
    },
    "id": "9DaIq39LU3J3"
   },
   "outputs": [],
   "source": [
    "sales = pd.read_csv('data/EV_sales_norway.csv',sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "executionInfo": {
     "elapsed": 3936,
     "status": "ok",
     "timestamp": 1607299918453,
     "user": {
      "displayName": "sha jiang",
      "photoUrl": "",
      "userId": "03234591794427525118"
     },
     "user_tz": -60
    },
    "id": "yG3HAky8U3J3",
    "outputId": "97ec30fe-3bda-4b17-d11b-d892fcc95877"
   },
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "     year  week  other  electric    weekday  ev_share\n",
       "0    2010     1     17         0 2010-01-03  0.000000\n",
       "1    2010     2   2776         0 2010-01-10  0.000000\n",
       "2    2010     3   1953         0 2010-01-17  0.000000\n",
       "3    2010     4   2078         0 2010-01-24  0.000000\n",
       "4    2010     5   2873         0 2010-01-31  0.000000\n",
       "..    ...   ...    ...       ...        ...       ...\n",
       "572  2020    44   1749      1869 2020-11-01  0.516584\n",
       "573  2020    45    961      1291 2020-11-08  0.573268\n",
       "574  2020    46   1174      1435 2020-11-15  0.550019\n",
       "575  2020    47   1259      1591 2020-11-22  0.558246\n",
       "576  2020    48   1586      2084 2020-11-29  0.567847\n",
       "\n",
       "[577 rows x 6 columns]"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>year</th>\n      <th>week</th>\n      <th>other</th>\n      <th>electric</th>\n      <th>weekday</th>\n      <th>ev_share</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2010</td>\n      <td>1</td>\n      <td>17</td>\n      <td>0</td>\n      <td>2010-01-03</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2010</td>\n      <td>2</td>\n      <td>2776</td>\n      <td>0</td>\n      <td>2010-01-10</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2010</td>\n      <td>3</td>\n      <td>1953</td>\n      <td>0</td>\n      <td>2010-01-17</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2010</td>\n      <td>4</td>\n      <td>2078</td>\n      <td>0</td>\n      <td>2010-01-24</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2010</td>\n      <td>5</td>\n      <td>2873</td>\n      <td>0</td>\n      <td>2010-01-31</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>572</th>\n      <td>2020</td>\n      <td>44</td>\n      <td>1749</td>\n      <td>1869</td>\n      <td>2020-11-01</td>\n      <td>0.516584</td>\n    </tr>\n    <tr>\n      <th>573</th>\n      <td>2020</td>\n      <td>45</td>\n      <td>961</td>\n      <td>1291</td>\n      <td>2020-11-08</td>\n      <td>0.573268</td>\n    </tr>\n    <tr>\n      <th>574</th>\n      <td>2020</td>\n      <td>46</td>\n      <td>1174</td>\n      <td>1435</td>\n      <td>2020-11-15</td>\n      <td>0.550019</td>\n    </tr>\n    <tr>\n      <th>575</th>\n      <td>2020</td>\n      <td>47</td>\n      <td>1259</td>\n      <td>1591</td>\n      <td>2020-11-22</td>\n      <td>0.558246</td>\n    </tr>\n    <tr>\n      <th>576</th>\n      <td>2020</td>\n      <td>48</td>\n      <td>1586</td>\n      <td>2084</td>\n      <td>2020-11-29</td>\n      <td>0.567847</td>\n    </tr>\n  </tbody>\n</table>\n<p>577 rows × 6 columns</p>\n</div>"
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "source": [
    "sales['weekday'] = ''\n",
    "pd.to_datetime(sales['weekday'])\n",
    "for index, row in sales.iterrows():\n",
    "    # minus 1 because the dataset week system goes from 1 to 53 instead of 0 to 52, day start from sunday 0 to macth google trend\n",
    "    sales.weekday[index] = datetime.strptime('{} {} 0'.format(row.year, row.week-1), '%Y %W %w').strftime(\"%Y-%m-%d\") \n",
    "sales.weekday = pd.to_datetime(sales.weekday, format='%Y-%m-%d')\n",
    "#compute market share for electric cars\n",
    "sales['ev_share'] = sales['electric']/(sales['other'] + sales['electric'])\n",
    "sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hNgumZreU3J6"
   },
   "source": [
    "### Step 1.2 - Google trend data import and wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function goal:      Convert start and end date to a string of dates with specific format\n",
    "def convert_to_date(satrt,end):\n",
    "      start = satrt.strftime(\"%Y-%m-%d\")\n",
    "      end = end.strftime(\"%Y-%m-%d\")\n",
    "      dates = start + ' ' + end\n",
    "      return dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function goal:      Split timespan in years\n",
    "Function inputs:    date_start\n",
    "                    date_end\n",
    "Function outputs:   years_expand = Array years\n",
    "'''\n",
    "\n",
    "def split_data_into_year(date_start, date_end):\n",
    "    years_expand = []\n",
    "    start_year = date_start\n",
    "    while abs(rdelta.relativedelta(start_year,date_end).years) >0 :\n",
    "      one_year = rdelta.relativedelta(weeks=+50)\n",
    "      end_year = start_year + one_year\n",
    "    \n",
    "      years_expand += [convert_to_date(start_year,end_year)]\n",
    "\n",
    "      #update start_year\n",
    "      one_week = rdelta.relativedelta(weeks=+1)\n",
    "      start_year = end_year + one_week\n",
    "\n",
    "    years_expand += [convert_to_date(start_year,date_end)]\n",
    "    return years_expand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function goal:      Import Google Trends data\n",
    "Function inputs:    kw = Keyword list \n",
    "                    geo = Country abreviation to search\n",
    "                    dates = Timespan to search\n",
    "                    eps = maximum propensity difference in matching\n",
    "Function outputs:   pytrends = full pytrends format dataset\n",
    "                    interest_over_time_df = Dataframe containing the interest evolution for each keyword\n",
    "'''\n",
    "\n",
    "def build_GT_data(kw, geo, dates):\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            pytrends.build_payload(kw_list=kw, cat=0, timeframe=dates, geo=geo, gprop='')\n",
    "            break\n",
    "        except:    \n",
    "            pass # Sometimes Google Trends rejects the request. Try again until it works ! \n",
    "\n",
    "    interest_over_time_df = pytrends.interest_over_time()\n",
    "\n",
    "    return interest_over_time_df[kw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function goal:      Normalise the weekly values. It finds the date and keyword with the max value in weekly. \n",
    "                    Looks for the value in the monthly dataset which is smaller and normalises all the weekly value to this one.\n",
    "Function inputs:    data_weekly = 1 year of Google Trends data with weekly granularity\n",
    "                    data_monthly = Whole deseired timespan of Google Trends data with monthly granularity\n",
    "                    kw = keywords\n",
    "Function outputs:   data_weekly = same as input but normalised \n",
    "'''\n",
    "def normalise(data_weekly, data_monthly, kw):\n",
    "    # 1) Convert weekly into monthly with mean()\n",
    "    data = data_weekly.groupby(data_weekly.index.strftime(\"%Y-%m\")).mean()\n",
    "\n",
    "    # 2) Find row index and column name of max value (which is not 100 as we have done the mean over 4 weeks)\n",
    "    yearly_max = data.max().max()\n",
    "    max_column = data.max().idxmax()\n",
    "    max_row = data[max_column].idxmax()\n",
    "\n",
    "    # 3) For the whole timespan (with monhtly datapoints), the value at the weekly max is 'true_max' \n",
    "    true_max = data_monthly[max_column].loc[data_monthly.index > max_row].iloc[0]\n",
    "\n",
    "    # 4) Normalise each weekly datapoints by this true_max/yearly_max in %\n",
    "    data_weekly = data_weekly*(true_max/yearly_max)/100\n",
    "\n",
    "    return data_weekly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Function goal:      Split the timespan in years so that we can extract weekly datapoints. \n",
    "Function inputs:    data_weekly = 1 year of Google Trends data with weekly granularity\n",
    "                    interest_monthly = Whole deseired timespan of Google Trends data with monthly granularity\n",
    "                    kw = keywords\n",
    "                    yrs = full timespan split into years 'start_date end_date' for each year as a list\n",
    "                    geo = Country abreviation to search\n",
    "Function outputs:   interest = full timespan interest with weekly datapoints\n",
    "'''\n",
    "def get_weekly_interest(interest_monthly, kw, yrs, geo):\n",
    "    interest = pd.DataFrame()\n",
    "    for yearly_dates in yrs:\n",
    "        interest_weekly = build_GT_data(kw, geo, yearly_dates)\n",
    "        \n",
    "        #if the search is for very short timespan it will result in daily instead of weekly values. So we take the weekly mean in case this happens\n",
    "        interest_weekly['date'] = pd.to_datetime(interest_weekly.index) - pd.to_timedelta(7, unit='d')\n",
    "        interest_weekly = interest_weekly.groupby(pd.Grouper(key='date', freq='W-SUN')).mean()\n",
    "        \n",
    "        interest_weekly = normalise(interest_weekly, interest_monthly, kw)\n",
    "        interest = pd.concat([interest, interest_weekly])\n",
    "    #The max week might actually not be around the same date as the max month. Hence we need to normalise again here so that max value is 100\n",
    "    interest = interest/interest.max().max()*100\n",
    "    return interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "UTC_offset = 60 # Minutes for Norway (UTC+1)\n",
    "host_language = 'en_US'\n",
    "# keywords related to category: economical \n",
    "kw_eco = [\"Elbil bompenger\", \"Elbil avgift\", \"Elbil pris\", \"Elbil fordeler\", \"Elbil parkering\"] \n",
    "# keywords related to category: EV models (top 10 EV models sold in Norway with Tesla models grouped together as 'Tesla')\n",
    "kw_evm = [\"Tesla\", \"Nissan Leaf\",\"Volkswagen e-Golf\", \"Bmw I3\", \"Kia Soul\"] \n",
    "# keywords related to category: range anxiety \n",
    "kw_ran = [\"Elbil lading\", \"Elbil ladestasjoner\", \"Ladestasjoner\", \"Hurtiglading\", \"Elbil rekkevidde\"] \n",
    "# keywords related to category: environement \n",
    "kw_env = [\"CO2 bil\", \"Bil utslipp\", \"Global oppvarming\", \"Bil miljø\", \"Karbonavtrykk\"]\n",
    "# all keywords grouped\n",
    "kw_all = kw_eco + kw_evm + kw_ran + kw_env\n",
    "date_start = date(2010,1,3)\n",
    "date_end = date(2020,1,5)\n",
    "dates = convert_to_date(date_start,date_end)\n",
    "yrs = split_data_into_year(date_start, date_end) # Will be usefull to extract yearly datasets to have weekly instead of monthly datapoints\n",
    "country_abbreviation = 'NO' #Norway abreviation\n",
    "pytrends = TrendReq(hl=host_language, tz=UTC_offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: get non granular over results for reference:\n",
    "# Giving a too large timespan yields unfortunately only monthy results. This will however serve as our reference frame for normalisation\n",
    "interest_monthly_kw_eco = build_GT_data(kw_eco, country_abbreviation, dates)\n",
    "interest_monthly_kw_evm = build_GT_data(kw_evm, country_abbreviation, dates)\n",
    "interest_monthly_kw_ran = build_GT_data(kw_ran, country_abbreviation, dates)\n",
    "interest_monthly_kw_env = build_GT_data(kw_env, country_abbreviation, dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 2: get weekly \n",
    "interest_kw_eco = get_weekly_interest(interest_monthly_kw_eco, kw_eco, yrs, country_abbreviation)\n",
    "interest_kw_evm = get_weekly_interest(interest_monthly_kw_evm, kw_evm, yrs, country_abbreviation)\n",
    "interest_kw_ran = get_weekly_interest(interest_monthly_kw_ran, kw_ran, yrs, country_abbreviation)\n",
    "interest_kw_env = get_weekly_interest(interest_monthly_kw_env, kw_env, yrs, country_abbreviation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 16028,
     "status": "ok",
     "timestamp": 1607299930628,
     "user": {
      "displayName": "sha jiang",
      "photoUrl": "",
      "userId": "03234591794427525118"
     },
     "user_tz": -60
    },
    "id": "BBMOku8daMw5"
   },
   "outputs": [],
   "source": [
    "sales_base = sales.copy()\n",
    "\n",
    "# convert rows to month or year where each row is 1 week\n",
    "month_delay = 4\n",
    "year_delay = 52\n",
    "\n",
    "#a fix - t-1 should not be 4, which would imply we are trying to use last years week, and four weeks past in order to predict NOW\n",
    "week_delay = 1\n",
    "\n",
    "sales_base['ev_share_tm1']=sales_base.ev_share.shift(week_delay)\n",
    "sales_base['ev_share_tm12']=sales_base.ev_share.shift(year_delay)\n",
    "sales_base['log_ev_share'] = np.log(sales_base.ev_share + 0.1) # We wish to avoid log(0) by shifting the points slightly\n",
    "sales_base['log_ev_share_tm1']=sales_base.log_ev_share.shift(week_delay)\n",
    "sales_base['log_ev_share_tm12']=sales_base.log_ev_share.shift(year_delay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataframes for statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sales = sales_base.iloc[:528,:].drop_duplicates(subset='weekday').set_index('weekday')\n",
    "df_inter_eco = interest_kw_eco.iloc[1:,:]\n",
    "df_inter_evm = interest_kw_evm.iloc[1:,:]\n",
    "df_inter_ran = interest_kw_ran.iloc[1:,:]\n",
    "df_inter_env = interest_kw_env.iloc[1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter = df_inter_eco.merge(df_inter_evm, left_index=True, right_index=True)\n",
    "df_inter = df_inter.merge(df_inter_ran, left_index=True, right_index=True)\n",
    "df_inter = df_inter.merge(df_inter_env, left_index=True, right_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle dataframes"
   ]
  },
  {
   "source": [
    "Final DataFrames to store in pickle and be used by the analysis notebook."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_inter.to_pickle('data/pytrends.pickle')\n",
    "df_sales.to_pickle('data/sales.pickle')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Norway_EV_sales.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}